LSTM flowchart

function preprocess_import_data(filename, num_time_steps){
  Load CSV with *filename*;
  // Restructure data for LSTM
  Separate rows into individual gesture samples;
  Get number of columns;
  // (num_samples x num_time_steps x num_columns)
  Restructure data into 3D tensor;
  // x includes 11 columns for all sensor data
//y_labels are gesture numbers scaled for zero-based array indexing
  
  Get x and y_labels;
  
  return x, y_labels;
}



Start;
Import libraries;
Set *num_time_steps*, * num_features* and *num_gestures*;
Set *filename* for importing gesture data[filename, num_time_steps];
call preprocess_import_data(filename, num_time_steps);
goto x_and_y_labels[x,y_labels];
x_and_y_labels:
Split data into training and testing sets;
block **Normalising the data**{
  Flatten both sets to 2D;
  || minMaxScaler||;
  goto normalise[normalised_data];
  normalise:
  Reshape back to 3D;
  branch(Export_scale_factors){
    Save scaling parameters;
  }
}
block **Creating the LSTM Sequential Model**{
|| Create sequential model ||;
// Takes input_shape of 1 x *num_time_steps*  x *num_features*
Add LSTM layer to model[*softmax* activation function];
Add dense layer to model with outputs equal to *num_gestures*[*Adam* optimiser,
*sparse_categorical_crossentropy* loss];
Compile model[Epochs,
Batch size,
callbacks = reduce_lr , early_stop];
}
Fit normalised data;
Make predictions on testing set;
Assign class label with highest prediction score;
for(Index=0;Index <= length(test_data);Index++){
  Print actual and predicted class for current gesture;
}
Evaluate model loss and accuracy;
Save model;
